from airflow import DAG
import airflow.providers.amazon
from datetime import timedelta, datetime
from airflow.providers.http.sensors.http import HttpSensor
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import json
import pandas as pd
import os


default_args = {
    'owner':'airflow',
    'depends_on_past':False,
    'start_date':datetime(2023,10,12),
    'email':['caguoji@gmail.com'],
    'email_on_failure':False,
    'email_on_retry':False
}



def download_from_s3(bucket_name,local_path):
    hook = S3Hook('s3_conn')
    files = hook.list_keys(bucket_name=bucket_name)
    for file in files:
        hook.download_file(key=file
        ,bucket_name=bucket_name
        ,local_path=local_path
        ,preserve_file_name=True
        ,use_autogenerated_subdir=False)

    return files   


with DAG('rolex_pipeline',
        default_args= default_args,
        catchup = False ) as dag:


        task_download_from_s3 = PythonOperator(
            task_id= 'download_from_s3',
            python_callable = download_from_s3,
            op_kwargs = {
                'bucket_name':'test-scrapy-rolex',
                'local_path':'/home/ubuntu/airflow/data/'   
            }
        )